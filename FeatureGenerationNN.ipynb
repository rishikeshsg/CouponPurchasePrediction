{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishikesh/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/rishikesh/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using gpu device 0: GeForce 920M (CNMeM is disabled, cuDNN 5105)\n",
      "/home/rishikesh/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.objectives import categorical_crossentropy, aggregate\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coupons_train=pd.read_csv(\"Translated_Data/coupon_list_train.csv\", parse_dates=[\"DISPFROM\",\"DISPEND\"])\n",
    "coupons_test = pd.read_csv(\"Translated_Data/coupon_list_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishikesh/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "coupons_train[\"DISPFROM\"].fillna(pd.Timestamp(\"19000101\"), inplace=True)\n",
    "coupons_train = coupons_train.sort(columns=[\"DISPFROM\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coupons_visit = pd.read_csv(\"Translated_Data/coupon_visit_train.csv\", parse_dates=[\"I_DATE\"])\n",
    "coupons_visit = coupons_visit.sort(columns=[\"I_DATE\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coupons_visit = coupons_visit.rename(columns={'VIEW_COUPON_ID_hash': 'COUPON_ID_hash'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_validation(coupons_train,time_delta):\n",
    "    max_date = coupons_train[\"DISPFROM\"].max()\n",
    "    valid_start = max_date - time_delta\n",
    "    coupons_valid = coupons_train[(coupons_train[\"DISPFROM\"] > valid_start)]\n",
    "    coupons_train = coupons_train[~ (coupons_train[\"DISPFROM\"] > valid_start)]\n",
    "    return coupons_train,coupons_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coupons_train,coupons_valid=gen_validation(coupons_train,datetime.timedelta(days=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(coupons_train,coupons_valid):\n",
    "    if len(coupons_valid)>0:\n",
    "        very_long_time_display=coupons_valid[coupons_valid.DISPPERIOD > 20].COUPON_ID_hash\n",
    "        very_low_price = coupons_valid[coupons_valid.DISCOUNT_PRICE <= 100].COUPON_ID_hash\n",
    "        coupons_valid = coupons_valid[~coupons_valid.COUPON_ID_hash.isin(very_long_time_display)]\n",
    "        coupons_valid = coupons_valid[~coupons_valid.COUPON_ID_hash.isin(very_low_price)].reset_index(drop=True)\n",
    "        \n",
    "    very_long_time_display = coupons_train[coupons_train.DISPPERIOD > 20].COUPON_ID_hash\n",
    "    coupons_train = coupons_train[~coupons_train.COUPON_ID_hash.isin(very_long_time_display)].reset_index(drop=True)\n",
    "    \n",
    "    return coupons_train,coupons_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coupons_train,coupons_valid=remove_outliers(coupons_train,coupons_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coupons_train[\"PREF_NAME\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_frame[\"PREF_NAME\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def preprocess(df):\n",
    "        df[\"REDUCE_PRICE\"] = df[\"CATALOG_PRICE\"] - df[\"DISCOUNT_PRICE\"]\n",
    "        for key in [\"DISCOUNT_PRICE\", \"CATALOG_PRICE\", \"REDUCE_PRICE\"]:\n",
    "            df[key + \"_LOG\"] = np.log(df[key] + 1.0).astype(np.float32)\n",
    "\n",
    "        df[\"VALIDPERIOD_NA\"] = np.array(pd.isnull(df[\"VALIDPERIOD\"]), dtype=np.int32)\n",
    "        df[\"DISPPERIOD_C\"] = np.array(df[\"DISPPERIOD\"].clip(0, 8), dtype=np.int32)\n",
    "        df[\"PRICE_RATE\"] = np.array(df.PRICE_RATE, dtype=np.float32)\n",
    "        df[\"large_area_name\"].fillna(\"NA\", inplace=True)\n",
    "        df[\"ken_name\"].fillna(\"NA\", inplace=True)\n",
    "        df[\"small_area_name\"].fillna(\"NA\", inplace=True)\n",
    "        df[\"LARGE_AREA_NAME\"] = df[\"large_area_name\"]\n",
    "        df[\"PREF_NAME\"] = df[\"ken_name\"]\n",
    "        df[\"SMALL_AREA_NAME\"] = df[\"large_area_name\"] + \":\" + df[\"ken_name\"] + \":\" + df[\"small_area_name\"]\n",
    "        df[\"CATEGORY_NAME\"] = df[\"CAPSULE_TEXT\"] + df[\"GENRE_NAME\"]\n",
    "\n",
    "        usable_dates = ['USABLE_DATE_MON',\n",
    "                        'USABLE_DATE_TUE',\n",
    "                        'USABLE_DATE_WED',\n",
    "                        'USABLE_DATE_THU',\n",
    "                        'USABLE_DATE_FRI',\n",
    "                        'USABLE_DATE_SAT',\n",
    "                        'USABLE_DATE_SUN',\n",
    "                        'USABLE_DATE_HOLIDAY',\n",
    "                        'USABLE_DATE_BEFORE_HOLIDAY']        \n",
    "        for key in usable_dates:\n",
    "            df[key].fillna(0, inplace=True)\n",
    "        \n",
    "        df[\"USABLE_DATE_SUM\"] = 0\n",
    "        for key in usable_dates:\n",
    "            df[\"USABLE_DATE_SUM\"] += df[key]\n",
    "        \n",
    "        cols = df.columns.tolist()\n",
    "        cols.remove(\"DISPFROM\")\n",
    "        cols.remove(\"DISPEND\")\n",
    "        for key in cols:\n",
    "            df[key].fillna(\"NA\", inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coupons_train=preprocess(coupons_train)\n",
    "coupons_valid=preprocess(coupons_valid)\n",
    "coupons_test=preprocess(coupons_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " coupon_mapper = DataFrameMapper([\n",
    "                ('CATEGORY_NAME', LabelBinarizer()),\n",
    "                ('PRICE_RATE', None),\n",
    "                ('CATALOG_PRICE_LOG', None),\n",
    "                ('DISCOUNT_PRICE_LOG', None),\n",
    "                ('REDUCE_PRICE_LOG', None),\n",
    "                ('DISPPERIOD_C', LabelBinarizer()),\n",
    "                ('VALIDPERIOD_NA', LabelBinarizer()),\n",
    "                ('USABLE_DATE_SUM', None),\n",
    "                ('LARGE_AREA_NAME', LabelBinarizer()),\n",
    "                ('SMALL_AREA_NAME', LabelBinarizer()),\n",
    "                ('PREF_NAME', LabelBinarizer()),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coupon_mapper.fit(pd.concat([coupons_train, coupons_valid, coupons_test]))\n",
    "train_coupon_vec = coupon_mapper.transform(coupons_train.copy())\n",
    "#valid_coupon_vec = coupon_mapper.transform(coupons_valid.copy())\n",
    "test_coupon_vec = coupon_mapper.transform(coupons_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len PREF_NAME = 47 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishikesh/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "user_frame = pd.read_csv(\"Translated_Data/user_list.csv\")\n",
    "details_frame = pd.read_csv(\"Translated_Data/coupon_detail_train.csv\",parse_dates=[\"I_DATE\"])\n",
    "details_frame = details_frame.sort(columns=[\"I_DATE\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_mapper = DataFrameMapper([\n",
    "                ('SEX_ID', LabelBinarizer()),\n",
    "                ('PREF_NAME', LabelBinarizer()),\n",
    "                ('AGE', None),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_frame[\"PREF_NAME\"].fillna(\"NA\", inplace=True)\n",
    "user_vec = user_mapper.fit_transform(user_frame.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_frame['PREF_NAME'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(user_vec[0][:])):\n",
    "    if user_vec[0][i] == 1:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = []\n",
    "coupons_train[\"ROW_ID\"] = pd.Series(coupons_train.index.tolist())\n",
    "#coupons_valid[\"ROW_ID\"] = pd.Series(coupons_valid.index.tolist())\n",
    "\n",
    "for i, user in user_frame.iterrows():\n",
    "    if i % 100 == 0:\n",
    "        sys.stdout.write(\"\\r{0}\".format(i))\n",
    "        sys.stdout.flush() \n",
    "    coupons = details_frame[details_frame.USER_ID_hash.isin([user[\"USER_ID_hash\"]])]\n",
    "    train_coupon_data = pd.merge(coupons[[\"COUPON_ID_hash\",\"ITEM_COUNT\",\"I_DATE\"]],\n",
    "                                         coupons_train,\n",
    "                                         on=\"COUPON_ID_hash\", how='inner',\n",
    "                                         suffixes=[\"_x\",\"\"], copy=False)\n",
    "    train_coupon_data = train_coupon_data.sort(columns=[\"I_DATE\"])\n",
    "    row_ids = train_coupon_data.ROW_ID.unique().tolist()\n",
    "    `\n",
    "    #coupons_visited = coupons_visit[coupons_visit.USER_ID_hash.isin([user[\"USER_ID_hash\"]])]\n",
    "    #train_coupon_visited_data = pd.merge(coupons_visited[[\"COUPON_ID_hash\",\"PURCHASE_FLG\"]],\n",
    "                                         #coupons_train,\n",
    "                                         #on=\"COUPON_ID_hash\", how='inner',\n",
    "                                         #suffixes=[\"_x\",\"\"], copy=False)\n",
    "    #train_coupon_visited_data = train_coupon_visited_data.sort(columns=[\"I_DATE\"])\n",
    "    #tmp = train_coupon_visited_data[train_coupon_visited_data.PURCHASE_FLG>0].ROW_ID.unique()\n",
    "    #visited_row_ids = [i for i in train_coupon_visited_data.ROW_ID.unique() if i not in tmp]\n",
    "    \n",
    "\n",
    "    #valid_coupon_data = pd.merge(coupons[[\"COUPON_ID_hash\",\"ITEM_COUNT\",\"I_DATE\"]],\n",
    "                                         #coupons_valid, on=\"COUPON_ID_hash\",\n",
    "                                         #how='inner', suffixes=[\"_x\",\"\"], copy=False)\n",
    "    #valid_coupon_data = valid_coupon_data.sort(columns=[\"I_DATE\"])\n",
    "    #valid_row_ids = valid_coupon_data.ROW_ID.unique().tolist()\n",
    "\n",
    "    users.append({\"user\": user_vec[i],\n",
    "                    \"coupon_ids\": row_ids})\n",
    "                    #\"valid_coupon_ids\": valid_row_ids,\n",
    "                    #\"visited_row_ids\": visited_row_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Used to load user so that don't have to generate again\n",
    "f = open('user_all.obj','rb')\n",
    "users = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coupon_ids': [15101],\n",
       " 'user': array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bucketizing the age.\n",
    "Does not work. \n",
    "\n",
    "Dont run the next 2 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BUCKET = 6\n",
    "def bucket(age):\n",
    "    r = np.zeros(NUM_BUCKET)\n",
    "    if age < 30:\n",
    "        r[0] = 1\n",
    "    elif age > 50:\n",
    "        r[-1] = 1\n",
    "    else:\n",
    "        r[(age/5)-5] = 1\n",
    "    #r[age/10-1] = 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[1]['user'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(users)):\n",
    "    users[i]['user'] = np.append(users[i]['user'][:-1],bucket(users[i]['user'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Storing user array for further use\n",
    "f = open('user_mod_all.obj', 'wb')\n",
    "cPickle.dump(users, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxmin_columns(coupons_train,coupon_ids):\n",
    "    return coupons_train.ix[\n",
    "        coupon_ids, (\"CATALOG_PRICE\",\"DISCOUNT_PRICE\")\n",
    "        ].as_matrix().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added mean of unpurchased coupon\n",
    "\n",
    "Number of unpurchased coupons\n",
    "\n",
    "Ratio of purchased to unpurchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def purchase_history_features(train_coupon_vec, user_coupon_vec, user_coupon_not_purchased_vec, maxmin_columns, \n",
    "                              filter_idx=None, n_filter_idx=None):\n",
    "        sum_vec = np.zeros(2, dtype=np.float32)\n",
    "        maxmin_vec = np.zeros((4), dtype=np.float32)\n",
    "        mean_coupon_vec = np.zeros(len(train_coupon_vec[0]), dtype=np.float32)\n",
    "        #mean_coupon_not_purchased_vec = np.zeros(len(train_coupon_vec[0]), dtype=np.float32)\n",
    "        \n",
    "        #lennp = 0\n",
    "        \n",
    "        if filter_idx is not None:\n",
    "            '''\n",
    "            if len(user_coupon_not_purchased_vec[n_filter_idx]) > 0:\n",
    "                lennp = len(user_coupon_not_purchased_vec[n_filter_idx])\n",
    "                mean_coupon_not_purchased_vec[:] = user_coupon_not_purchased_vec[n_filter_idx].mean(0)\n",
    "            '''\n",
    "            if len(user_coupon_vec[filter_idx]) > 0:\n",
    "                mean_coupon_vec[:] = user_coupon_vec[filter_idx].mean(0)\n",
    "                sum_vec[0] = filter_idx.sum()\n",
    "                sum_vec[1] = np.log(sum_vec[0] + 1.0)\n",
    "                max_val = maxmin_columns[filter_idx].max(0)\n",
    "                min_val = maxmin_columns[filter_idx].min(0)\n",
    "                maxmin_vec[0] = max_val[0]\n",
    "                maxmin_vec[1] = min_val[0]\n",
    "                maxmin_vec[2] = max_val[1]\n",
    "                maxmin_vec[3] = min_val[1]\n",
    "        else:\n",
    "            '''\n",
    "            if len(user_coupon_not_purchased_vec) > 0:\n",
    "                lennp = len(user_coupon_not_purchased_vec)\n",
    "                mean_coupon_not_purchased_vec[:] = user_coupon_not_purchased_vec.mean(0)\n",
    "            '''\n",
    "            if len(user_coupon_vec) > 0:\n",
    "                mean_coupon_vec = user_coupon_vec.mean(0)\n",
    "                sum_vec[0] = len(user_coupon_vec)\n",
    "                sum_vec[1] = np.log(sum_vec[0] + 1.0)\n",
    "                max_val = maxmin_columns.max(0)\n",
    "                min_val = maxmin_columns.min(0)\n",
    "                maxmin_vec[0] = max_val[0]\n",
    "                maxmin_vec[1] = min_val[0]\n",
    "                maxmin_vec[2] = max_val[1]\n",
    "                maxmin_vec[3] = min_val[1]\n",
    "        '''        \n",
    "        ratio = 0\n",
    "        if lennp != 0:\n",
    "            ratio = sum_vec[0]*1.0/lennp\n",
    "        '''\n",
    "        #return np.hstack((mean_coupon_vec, mean_coupon_not_purchased_vec, sum_vec, maxmin_vec, \n",
    "                          #lennp, ratio))\n",
    "        return np.hstack((sum_vec, maxmin_vec)),mean_coupon_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COUPON_DISP_NEAR = 400\n",
    "COUPON_DISP_NEAR_MIN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_data(num_nega=2, verbose=True):\n",
    "        x = []\n",
    "        y = []\n",
    "        count = 0\n",
    "        for user in users:\n",
    "            if count % 100 == 0:\n",
    "                sys.stdout.write(\"\\r{0}/{1}\".format(count, len(users)))\n",
    "                sys.stdout.flush()\n",
    "            count += 1\n",
    "            coupon_ids = np.array(user[\"coupon_ids\"], dtype=np.int32)\n",
    "            user_coupons = train_coupon_vec[coupon_ids]\n",
    "            \n",
    "            '''\n",
    "            coupon_ids_np = np.array(user[\"visited_row_ids\"], dtype=np.int32)\n",
    "            user_coupon_np = train_coupon_vec[coupon_ids_np]\n",
    "            '''\n",
    "            maxmin_c = maxmin_columns(coupons_train,coupon_ids)\n",
    "            for i in xrange(len(user_coupons)):\n",
    "                target_coupon_vec = user_coupons[i]\n",
    "                rid = coupon_ids[i]\n",
    "                nega_list = range(max(0, rid - COUPON_DISP_NEAR), rid)\n",
    "                if len(nega_list) < COUPON_DISP_NEAR_MIN:\n",
    "                    continue\n",
    "\n",
    "                filter_idx = np.ones(user_coupons.shape[0], dtype=np.bool)\n",
    "                \n",
    "                # exclude coupons that was purchased after the target coupon\n",
    "                filter_idx[i:] = False\n",
    "                # exclude the target coupon (and remove duplicate)\n",
    "                filter_idx[coupon_ids == coupon_ids[i]] = False\n",
    "               \n",
    "                '''\n",
    "                filter_idx_np = np.ones(user_coupon_np.shape[0], dtype=np.bool)\n",
    "                for ix in range(len(filter_idx_np)):\n",
    "                    if user_coupon_np[ix] in user_coupons:\n",
    "                        filter_idx_np[ix] = False\n",
    "                \n",
    "                index = np.where(coupon_ids_np == coupon_ids[i])[0]\n",
    "                if len(index) > 0:\n",
    "                    filter_idx_np[index[0]:] = False\n",
    "                filter_idx_np[coupon_ids_np in coupon_ids] = False\n",
    "                '''\n",
    "                hist_feat, mean = purchase_history_features(train_coupon_vec,user_coupons, user_coupons,\n",
    "                                                             maxmin_c,\n",
    "                                                             filter_idx, filter_idx)\n",
    "                \n",
    "                # dot product(both elementwise and overall) for similarity \n",
    "                # between mean purchased coupons and current coupon\n",
    "                \n",
    "                sim_coupon_vec = mean*target_coupon_vec\n",
    "                sim_score = np.dot(mean, target_coupon_vec)\n",
    "                \n",
    "                #cos_sim = cosine_similarity(mean.reshape((-1,1)).T, target_coupon_vec.reshape((-1,1)).T)[0]\n",
    "                #euc_sim = euclidean_distances(mean.reshape((-1,1)).T, target_coupon_vec.reshape((-1,1)).T)[0]\n",
    "                \n",
    "                area_sim = np.dot(target_coupon_vec[-47:], np.delete(user[\"user\"][1:-6], 26))\n",
    "                #sim_coupon_np_vec = hist_feat[150:300]*target_coupon_vec\n",
    "                \n",
    "                # feature vector (user_feature + purchase_history_feature + coupon_feature)\n",
    "                purchased_feat = np.hstack((user[\"user\"], hist_feat, target_coupon_vec, \n",
    "                                            sim_coupon_vec, sim_score, area_sim))\n",
    "                                            #, sim_coupon_np_vec))\n",
    "                x.append(purchased_feat)\n",
    "                y.append([1]) # posi\n",
    "\n",
    "                # select random unpurchased coupons\n",
    "                for j in xrange(num_nega):\n",
    "                    found = False\n",
    "                    for _ in xrange(10):\n",
    "                        unpurchased_idx = np.random.choice(nega_list, 1)[0]\n",
    "                        if unpurchased_idx not in user[\"coupon_ids\"]:\n",
    "                            found = True\n",
    "                            break\n",
    "                    if found:\n",
    "                        \n",
    "                        sim_coupon_vec = mean*train_coupon_vec[unpurchased_idx]\n",
    "                        sim_score = np.dot(mean, train_coupon_vec[unpurchased_idx])\n",
    "                        \n",
    "                        #cos_sim = cosine_similarity(mean.reshape((-1,1)).T, \n",
    "                        #                            train_coupon_vec[unpurchased_idx].reshape((-1,1)).T)[0]\n",
    "                        #euc_sim = euclidean_distances(mean.reshape((-1,1)).T,\n",
    "                        #                            train_coupon_vec[unpurchased_idx].reshape((-1,1)).T)[0]\n",
    "                \n",
    "                        area_sim = np.dot(train_coupon_vec[unpurchased_idx][-47:], \n",
    "                                          np.delete(user[\"user\"][1:-6], 26))\n",
    "                        #sim_coupon_np_vec = hist_feat[150:300]*train_coupon_vec[unpurchased_idx]\n",
    "                        \n",
    "                        unpurchased_feat = np.hstack((user[\"user\"],\n",
    "                                                      hist_feat,\n",
    "                                                      train_coupon_vec[unpurchased_idx], \n",
    "                                                      sim_coupon_vec, sim_score,\n",
    "                                                      #euc_sim, cos_sim, \n",
    "                                                      area_sim))#, sim_coupon_np_vec))\n",
    "                        x.append(unpurchased_feat)\n",
    "                        y.append([0]) # nega\n",
    "                        \n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_valid_data(num_nega=5, verbose=True):\n",
    "        x = []\n",
    "        y = []\n",
    "        count = 0\n",
    "        for user in users:\n",
    "            if count % 100 == 0:\n",
    "                sys.stdout.write(\"\\r{0}/{1}\".format(count, len(users)))\n",
    "                sys.stdout.flush()\n",
    "            count += 1\n",
    "            coupon_ids = np.array(user[\"coupon_ids\"], dtype=np.int32)\n",
    "            user_coupons = train_coupon_vec[coupon_ids]\n",
    "            \n",
    "            '''\n",
    "            coupon_ids_np = np.array(user[\"visited_row_ids\"], dtype=np.int32)\n",
    "            user_coupon_np = train_coupon_vec[np.setdiff1d(coupon_ids_np,coupon_ids)]\n",
    "            '''\n",
    "            \n",
    "            valid_coupon_ids=np.array(user[\"valid_coupon_ids\"], dtype=np.int32)\n",
    "            valid_user_coupons=valid_coupon_vec[valid_coupon_ids]\n",
    "            maxmin_c = maxmin_columns(coupons_train,coupon_ids)\n",
    "            \n",
    "            hist_feat = purchase_history_features(train_coupon_vec, user_coupons, user_coupons, maxmin_c)\n",
    "            \n",
    "            for i in xrange(len(valid_user_coupons)):\n",
    "                target_coupon_vec = valid_user_coupons[i]\n",
    "                #rid = coupon_ids[i]\n",
    "                sim_coupon_vec = hist_feat[:150]*target_coupon_vec\n",
    "                sim_coupon_np_vec = hist_feat[150:300]*target_coupon_vec\n",
    "                #sim_score = np.dot(hist_feat[:150], target_coupon_vec)\n",
    "                \n",
    "                # feature vector (user_feature + purchase_history_feature + coupon_feature)\n",
    "                purchased_feat = np.hstack((user[\"user\"], hist_feat, target_coupon_vec,\n",
    "                                            sim_coupon_vec, sim_score))#, sim_coupon_np_vec))\n",
    "                x.append(purchased_feat)\n",
    "                y.append([1]) # posi\n",
    "\n",
    "                # select random unpurchased coupons\n",
    "                for j in xrange(num_nega):\n",
    "                    found = False\n",
    "                    for _ in xrange(10):\n",
    "                        unpurchased_idx = np.random.choice(range(len(valid_coupon_vec)), 1)[0]\n",
    "                        if unpurchased_idx not in user[\"valid_coupon_ids\"]:\n",
    "                            found = True\n",
    "                            break\n",
    "                    if found:\n",
    "                        sim_coupon_vec = hist_feat[:150]*valid_coupon_vec[unpurchased_idx]\n",
    "                        sim_score = np.dot(hist_feat[:150], valid_coupon_vec[unpurchased_idx])\n",
    "                        #sim_coupon_np_vec = hist_feat[150:300]*valid_coupon_vec[unpurchased_idx]\n",
    "                        unpurchased_feat = np.hstack((user[\"user\"],\n",
    "                                                      hist_feat,\n",
    "                                                      valid_coupon_vec[unpurchased_idx], \n",
    "                                                      sim_coupon_vec, sim_score))#, sim_coupon_np_vec))\n",
    "                        x.append(unpurchased_feat)\n",
    "                        y.append([0]) # nega\n",
    "\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17200/22873"
     ]
    }
   ],
   "source": [
    "train_feature,train_label=gen_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_feature,valid_label=gen_valid_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "f = open('train_label.obj', 'rb')\n",
    "train_label = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('valid_label.obj', 'rb')\n",
    "valid_label = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('train__mod_feat.obj', 'rb')\n",
    "train_feature = cPickle.load(f)\n",
    "f.close()\n",
    "f = open('train_mod_label.obj', 'rb')\n",
    "train_label = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('valid_feat.obj', 'rb')\n",
    "valid_feature = cPickle.load(f)\n",
    "f.close()\n",
    "f = open('valid_label.obj', 'rb')\n",
    "valid_label = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('train__mod_feat.obj', 'wb')\n",
    "cPickle.dump(train_feature, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "f = open('train_mod_label.obj', 'wb')\n",
    "cPickle.dump(train_label, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('valid_feat.obj', 'wb')\n",
    "cPickle.dump(valid_feature, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "f = open('valid_label.obj', 'wb')\n",
    "cPickle.dump(valid_label, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(459231, 363)\n"
     ]
    }
   ],
   "source": [
    "print train_feature.shape\n",
    "#print valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label = train_label.reshape((-1))\n",
    "valid_label = valid_label.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler.fit(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_feature = scaler.fit_transform(train_feature)\n",
    "#valid_feature = scaler.transform(valid_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, len(train_feature[0])),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    #l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.1)\n",
    "\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            #l_in_drop, \n",
    "            l_in, \n",
    "            num_units=1300,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)#,\n",
    "            #W=lasagne.init.Normal(std=1.0))\n",
    "\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1_drop,\n",
    "            #l_hid1, \n",
    "            num_units=80,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)#,\n",
    "            #W=lasagne.init.Normal())\n",
    "\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.1)\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2_drop,\n",
    "            #l_hid2, \n",
    "            num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.sigmoid)#,\n",
    "            #W=lasagne.init.Normal())\n",
    "\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n"
     ]
    }
   ],
   "source": [
    "input_var = T.matrix('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_mlp(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "\n",
    "#l2_penalty = regularize_layer_params_weighted(layers, l2)\n",
    "loss = lasagne.objectives.binary_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "train_predict_fn = theano.function([input_var], prediction)\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=1e-2, momentum=0.9)\n",
    "\n",
    "#updates = lasagne.updates.momentum(loss, params, learning_rate=0.01)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.binary_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(test_prediction >= 0.5, target_var))\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "test_predict_fn = theano.function([input_var], test_prediction >= 0.5)\n",
    "test_predict_probs_fn = theano.function([input_var], test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature = []\n",
    "train_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1 of 5 took 191.076s\n",
      "  training loss:\t\t0.472017\n",
      "  training accuracy:\t\t77.931375\n",
      "\n",
      "Epoch 2 of 5 took 191.739s\n",
      "  training loss:\t\t0.444714\n",
      "  training accuracy:\t\t78.938484\n",
      "\n",
      "Epoch 3 of 5 took 185.063s\n",
      "  training loss:\t\t0.435491\n",
      "  training accuracy:\t\t79.369294\n",
      "\n",
      "Epoch 4 of 5 took 182.493s\n",
      "  training loss:\t\t0.429568\n",
      "  training accuracy:\t\t79.643722\n",
      "\n",
      "Epoch 5 of 5 took 182.874s\n",
      "  training loss:\t\t0.425131\n",
      "  training accuracy:\t\t79.790084\n"
     ]
    }
   ],
   "source": [
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "sys.stdout.flush()\n",
    "num_epochs = 5\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    '''\n",
    "    if epoch % 5 == 0:\n",
    "        del train_feature\n",
    "        del train_label\n",
    "        train_feature,train_label=gen_train_data()\n",
    "        train_feature = scaler.transform(train_feature)\n",
    "    '''\n",
    "    for batch in iterate_minibatches(train_feature, train_label, 128, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        train_acc += acc\n",
    "        train_batches += 1\n",
    "        #break\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    '''\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(valid_feature, valid_label, 128, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "        #break\n",
    "    '''\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"\\nEpoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.6f}\".format(train_acc / train_batches * 100))\n",
    "    sys.stdout.flush()\n",
    "    '''\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    '''\n",
    "    '''\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        np.savez('models_area/model_all,1300,80,'+str(epoch+1)+',bothSim,noUnvisited,noSampling,noMean,areasim.npz', \n",
    "                 *lasagne.layers.get_all_param_values(network))\n",
    "        user_predictions={}\n",
    "        start = time.time()\n",
    "        predict_for_test()\n",
    "        with open('models_area/output'+str(epoch+1)+'.csv', 'w') as f:\n",
    "            f.write('USER_ID_hash,PURCHASED_COUPONS\\n')\n",
    "            for user in user_predictions.keys():\n",
    "                f.write(user+','+' '.join(user_predictions[user])+'\\n')\n",
    "        print  \n",
    "        print time.time() - start\n",
    "    '''\n",
    "    #    f = open('models/model_'+str(epoch)+'.obj', 'wb')\n",
    "    #    cPickle.dump(network, f, protocol=-1)\n",
    "    #    f.close()\n",
    "    #    np.savez('models/model_'+str(epoch)+'.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = 2\n",
    "D = [1/len(train_feature)]\n",
    "for i in range(n_estimators):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('models/model_all,1300,80,5,bothSim,noUnvisited,noSampling,NoMean.npz', \n",
    "         *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('models/model_100.obj', 'wb')\n",
    "cPickle.dump(network, f, protocol=-1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('models/model_25.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasagne.layers.get_all_param_values(network)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('models/model_20.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vl = test_predict_fn(valid_feature)\n",
    "err = 0\n",
    "c=0\n",
    "for i in xrange(len(vl)):\n",
    "    if vl[i] == 1:\n",
    "        c+=1\n",
    "    if valid_label[i] != vl[i]:\n",
    "        err += 1\n",
    "print err*1.0/len(vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for batch in iterate_minibatches(train_feature, train_label, 10, shuffle=True):\n",
    "    inputs, targets = batch\n",
    "    print test_predict_fn(inputs)\n",
    "    print targets\n",
    "    print test_predict_probs_fn(inputs)\n",
    "    print val_fn(inputs, targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_for_test() :\n",
    "    test_user_coupons=test_coupon_vec\n",
    "    user_id=0\n",
    "    for user in users:\n",
    "        coupon_ids = np.array(user[\"coupon_ids\"], dtype=np.int32)\n",
    "        user_coupons = train_coupon_vec[coupon_ids]\n",
    "        \n",
    "        '''\n",
    "        coupon_ids_np = np.array(user[\"visited_row_ids\"], dtype=np.int32)\n",
    "        user_coupon_np = train_coupon_vec[np.setdiff1d(coupon_ids_np,coupon_ids)]\n",
    "        '''\n",
    "        \n",
    "        maxmin_c = maxmin_columns(coupons_train,coupon_ids)\n",
    "        hist_feat, mean = purchase_history_features(train_coupon_vec, user_coupons, user_coupons, maxmin_c)\n",
    "        \n",
    "        sim_coupon_vec = mean*test_coupon_vec\n",
    "        sim_score = np.dot(mean,test_coupon_vec.T).reshape((-1,1))\n",
    "        \n",
    "        #cos_sim = cosine_similarity(mean.reshape((-1,1)).T, test_coupon_vec).T\n",
    "        #euc_sim = euclidean_distances(mean.reshape((-1,1)).T, test_coupon_vec).T\n",
    "        #print cos_sim.shape\n",
    "        #print euc_sim.shape\n",
    "        \n",
    "        area_sim = np.dot(np.delete(user[\"user\"][1:-6], 26), test_coupon_vec[:,-47:].T).reshape((-1,1))\n",
    "        #print area_sim.shape\n",
    "        #sim_coupon_np_vec = hist_feat[150:300]*test_coupon_vec\n",
    "        \n",
    "        user_feat=np.hstack((user[\"user\"],hist_feat))\n",
    "        user_feat_rep=np.array([list(user_feat)]*len(test_coupon_vec))\n",
    "        \n",
    "        purchased_feats=np.hstack((user_feat_rep,test_coupon_vec,sim_coupon_vec, sim_score,  \n",
    "                                   #euc_sim, cos_sim, \n",
    "                                   area_sim))\n",
    "        \n",
    "        purchased_feats = scaler.transform(purchased_feats)\n",
    "        ## TODO predict using purchased_feats\n",
    "        conf_predicted = test_predict_probs_fn(purchased_feats)\n",
    "        #print conf_predicted\n",
    "        #print test_predict_fn(purchased_feats)\n",
    "        #conf_predicted = train_predict_fn(purchased_feats)\n",
    "        \n",
    "        conf_predicted=[(conf_predicted[i],i) for i in range(len(conf_predicted))]\n",
    "        conf_predicted.sort()\n",
    "        top10=[ i for (c,i) in conf_predicted[-10:] ]\n",
    "        top10.reverse()\n",
    "        \n",
    "        #print top10\n",
    "        \n",
    "        user_predictions[user_frame['USER_ID_hash'][user_id]]=[coupons_test['COUPON_ID_hash'][i] for i in top10]\n",
    "        user_id+=1\n",
    "        \n",
    "        #if user_id == 2:\n",
    "        #break\n",
    "        \n",
    "        if user_id%100==0:\n",
    "            sys.stdout.write(\"\\r{0}/{1}\".format(user_id, len(users)))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22800/22873\n",
      "907.143246889\n"
     ]
    }
   ],
   "source": [
    "user_predictions={}\n",
    "start = time.time()\n",
    "predict_for_test()\n",
    "print  \n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print len(user_predictions[user_predictions.keys()[0]])\n",
    "print len(user_predictions[user_predictions.keys()[1]])\n",
    "print len(user_predictions[user_predictions.keys()[2]])\n",
    "print len(user_predictions[user_predictions.keys()[3]])\n",
    "print len(user_predictions[user_predictions.keys()[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output_area_age_bucket.csv', 'w') as f:\n",
    "    f.write('USER_ID_hash,PURCHASED_COUPONS\\n')\n",
    "    for user in user_predictions.keys():\n",
    "        f.write(user+','+' '.join(user_predictions[user])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
